{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "class evaluate_metrics():\n",
    "\tdef rmse(self, y_true, y_pred):\n",
    "\t\treturn sqrt(self.mse(y_true, y_pred))\n",
    "\n",
    "\tdef mse(self, y_true, y_pred):\n",
    "\t\treturn mean_squared_error(y_true, y_pred)\n",
    "\n",
    "\tdef mae(self, y_true, y_pred):\n",
    "\t\treturn mean_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Function for transform data into supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function split data into supervised learning\n",
    "def split_sequence(series=None, labels=None, n_timesteps=24, train_length=254, val_length=54):\n",
    "    n = len(data)\n",
    "    \n",
    "    #Find length by multiple timestep with days\n",
    "    train_length  = train_length * n_timesteps\n",
    "    val_length = val_length * n_timesteps + train_length\n",
    "    \n",
    "    #Splitting\n",
    "    train_x, train_y = series[:train_length], labels[:train_length]\n",
    "    val_x, val_y = series[train_length: val_length], labels[train_length: val_length]\n",
    "    test_x, test_y = series[val_length:], labels[val_length:]\n",
    "    \n",
    "    #Split into sequence (24 timesteps)\n",
    "    train_x, train_y = array(split(train_x, len(train_x)/n_timesteps)), array(split(train_y, len(train_y)/n_timesteps))\n",
    "    val_x, val_y = array(split(val_x, len(val_x)/n_timesteps)), array(split(val_y, len(val_y)/n_timesteps))\n",
    "    test_x, test_y = array(split(test_x, len(test_x)/n_timesteps)), array(split(test_y, len(test_y)/n_timesteps))\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Power</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Smooth</th>\n",
       "      <th>Day_of_Week</th>\n",
       "      <th>Events</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01 00:00:00</th>\n",
       "      <td>627.3</td>\n",
       "      <td>67.0</td>\n",
       "      <td>537.965366</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01 01:00:00</th>\n",
       "      <td>624.8</td>\n",
       "      <td>63.0</td>\n",
       "      <td>524.488132</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01 02:00:00</th>\n",
       "      <td>704.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>620.592628</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01 03:00:00</th>\n",
       "      <td>768.5</td>\n",
       "      <td>63.0</td>\n",
       "      <td>692.804419</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01 04:00:00</th>\n",
       "      <td>728.2</td>\n",
       "      <td>62.0</td>\n",
       "      <td>702.827174</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-29 19:00:00</th>\n",
       "      <td>1363.7</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1533.936705</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-29 20:00:00</th>\n",
       "      <td>1305.8</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1476.069433</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-29 21:00:00</th>\n",
       "      <td>1288.8</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1331.001113</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-29 22:00:00</th>\n",
       "      <td>890.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>913.536397</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-29 23:00:00</th>\n",
       "      <td>812.5</td>\n",
       "      <td>62.0</td>\n",
       "      <td>745.102843</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8712 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Power  Temperature       Smooth  Day_of_Week  Events\n",
       "Datetime                                                                  \n",
       "2010-01-01 00:00:00   627.3         67.0   537.965366            4       0\n",
       "2010-01-01 01:00:00   624.8         63.0   524.488132            4       0\n",
       "2010-01-01 02:00:00   704.0         64.0   620.592628            4       0\n",
       "2010-01-01 03:00:00   768.5         63.0   692.804419            4       0\n",
       "2010-01-01 04:00:00   728.2         62.0   702.827174            4       0\n",
       "...                     ...          ...          ...          ...     ...\n",
       "2010-12-29 19:00:00  1363.7         64.0  1533.936705            2       0\n",
       "2010-12-29 20:00:00  1305.8         62.0  1476.069433            2       0\n",
       "2010-12-29 21:00:00  1288.8         63.0  1331.001113            2       0\n",
       "2010-12-29 22:00:00   890.0         62.0   913.536397            2       0\n",
       "2010-12-29 23:00:00   812.5         62.0   745.102843            2       0\n",
       "\n",
       "[8712 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "#Loading a dataset\n",
    "ds_path = '../dataset/clean/'\n",
    "filename = 'building1retail.csv'\n",
    "full_path = os.path.join(ds_path, filename)\n",
    "\n",
    "dataframe = pd.read_csv(full_path, header=0, index_col=[\"Datetime\"], parse_dates=True, infer_datetime_format=True, low_memory=False)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of papers mention that deep learning is working better when the data scale \n",
    "\n",
    "Therefore, in this experiment we are using min max scaler https://en.wikipedia.org/wiki/Feature_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Smooth</th>\n",
       "      <th>Day_of_Week</th>\n",
       "      <th>Events</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01 00:00:00</th>\n",
       "      <td>67.0</td>\n",
       "      <td>537.965366</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01 01:00:00</th>\n",
       "      <td>63.0</td>\n",
       "      <td>524.488132</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01 02:00:00</th>\n",
       "      <td>64.0</td>\n",
       "      <td>620.592628</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01 03:00:00</th>\n",
       "      <td>63.0</td>\n",
       "      <td>692.804419</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01 04:00:00</th>\n",
       "      <td>62.0</td>\n",
       "      <td>702.827174</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-29 19:00:00</th>\n",
       "      <td>64.0</td>\n",
       "      <td>1533.936705</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-29 20:00:00</th>\n",
       "      <td>62.0</td>\n",
       "      <td>1476.069433</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-29 21:00:00</th>\n",
       "      <td>63.0</td>\n",
       "      <td>1331.001113</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-29 22:00:00</th>\n",
       "      <td>62.0</td>\n",
       "      <td>913.536397</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-29 23:00:00</th>\n",
       "      <td>62.0</td>\n",
       "      <td>745.102843</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8712 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Temperature       Smooth  Day_of_Week  Events\n",
       "Datetime                                                          \n",
       "2010-01-01 00:00:00         67.0   537.965366            4       0\n",
       "2010-01-01 01:00:00         63.0   524.488132            4       0\n",
       "2010-01-01 02:00:00         64.0   620.592628            4       0\n",
       "2010-01-01 03:00:00         63.0   692.804419            4       0\n",
       "2010-01-01 04:00:00         62.0   702.827174            4       0\n",
       "...                          ...          ...          ...     ...\n",
       "2010-12-29 19:00:00         64.0  1533.936705            2       0\n",
       "2010-12-29 20:00:00         62.0  1476.069433            2       0\n",
       "2010-12-29 21:00:00         63.0  1331.001113            2       0\n",
       "2010-12-29 22:00:00         62.0   913.536397            2       0\n",
       "2010-12-29 23:00:00         62.0   745.102843            2       0\n",
       "\n",
       "[8712 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We want to use a previous 24 hours data predict next 24 hours data\n",
    "#Original power no longer use so we have to drop it\n",
    "dataframe = dataframe.drop(['Power'], axis=1)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaler data\n",
    "\n",
    "series = dataframe.values\n",
    "scaler = MinMaxScaler()\n",
    "scalered_data = scaler.fit_transform(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide into data and label\n",
    "\n",
    "data = scalered_data[:-24]\n",
    "labels = scalered_data[24:, 1] #We only need to predict a power consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into supervised learning\n",
    "\n",
    "train_x, train_y, val_x, val_y, test_x, test_y = split_sequence(data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "#Create a simple LSTM model\n",
    "# prepare data\n",
    "n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs))\n",
    "model.compile(loss='mse', metrics='mae', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 200)               164000    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                2424      \n",
      "=================================================================\n",
      "Total params: 186,524\n",
      "Trainable params: 186,524\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.3359 - mae: 0.5354 - val_loss: 0.1716 - val_mae: 0.3489\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.1225 - mae: 0.2791 - val_loss: 0.0471 - val_mae: 0.1776\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0351 - mae: 0.1442 - val_loss: 0.0083 - val_mae: 0.0628\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.0095 - mae: 0.0768 - val_loss: 0.0032 - val_mae: 0.0441\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0053 - mae: 0.0583 - val_loss: 0.0024 - val_mae: 0.0370\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0041 - mae: 0.0510 - val_loss: 0.0035 - val_mae: 0.0494\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0031 - mae: 0.0434 - val_loss: 0.0030 - val_mae: 0.0463\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0026 - mae: 0.0400 - val_loss: 0.0025 - val_mae: 0.0417\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0025 - mae: 0.0389 - val_loss: 0.0022 - val_mae: 0.0390\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0023 - mae: 0.0377 - val_loss: 0.0020 - val_mae: 0.0365\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0022 - mae: 0.0368 - val_loss: 0.0021 - val_mae: 0.0378\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.0022 - mae: 0.0363 - val_loss: 0.0021 - val_mae: 0.0380\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0021 - mae: 0.0356 - val_loss: 0.0020 - val_mae: 0.0363\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.0021 - mae: 0.0349 - val_loss: 0.0019 - val_mae: 0.0354\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.0020 - mae: 0.0349 - val_loss: 0.0020 - val_mae: 0.0358\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.0020 - mae: 0.0342 - val_loss: 0.0018 - val_mae: 0.0331\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.0019 - mae: 0.0334 - val_loss: 0.0019 - val_mae: 0.0352\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0019 - mae: 0.0332 - val_loss: 0.0018 - val_mae: 0.0330\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0018 - mae: 0.0325 - val_loss: 0.0026 - val_mae: 0.0430\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0018 - mae: 0.0320 - val_loss: 0.0025 - val_mae: 0.0421\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0017 - mae: 0.0313 - val_loss: 0.0017 - val_mae: 0.0325\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.0017 - mae: 0.0315 - val_loss: 0.0013 - val_mae: 0.0254\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.0017 - mae: 0.0317 - val_loss: 0.0016 - val_mae: 0.0300\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0017 - mae: 0.0308 - val_loss: 0.0019 - val_mae: 0.0338\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0017 - mae: 0.0307 - val_loss: 0.0023 - val_mae: 0.0392\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.0016 - mae: 0.0297 - val_loss: 0.0022 - val_mae: 0.0378\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0015 - mae: 0.0288 - val_loss: 0.0016 - val_mae: 0.0301\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.0014 - mae: 0.0274 - val_loss: 0.0020 - val_mae: 0.0349\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0014 - mae: 0.0279 - val_loss: 0.0018 - val_mae: 0.0324\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0014 - mae: 0.0277 - val_loss: 0.0016 - val_mae: 0.0287\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0014 - mae: 0.0284 - val_loss: 0.0022 - val_mae: 0.0379\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0014 - mae: 0.0282 - val_loss: 0.0019 - val_mae: 0.0345\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.0017 - mae: 0.0324 - val_loss: 0.0018 - val_mae: 0.0339\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0016 - mae: 0.0298 - val_loss: 0.0027 - val_mae: 0.0452\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0015 - mae: 0.0293 - val_loss: 0.0029 - val_mae: 0.0475\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0014 - mae: 0.0282 - val_loss: 0.0027 - val_mae: 0.0448\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0013 - mae: 0.0269 - val_loss: 0.0022 - val_mae: 0.0388\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0013 - mae: 0.0268 - val_loss: 0.0019 - val_mae: 0.0344\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0014 - mae: 0.0281 - val_loss: 0.0029 - val_mae: 0.0460\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0014 - mae: 0.0288 - val_loss: 0.0036 - val_mae: 0.0537\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0015 - mae: 0.0292 - val_loss: 0.0048 - val_mae: 0.0627\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0016 - mae: 0.0308 - val_loss: 0.0023 - val_mae: 0.0407\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0013 - mae: 0.0264 - val_loss: 0.0025 - val_mae: 0.0435\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0013 - mae: 0.0270 - val_loss: 0.0017 - val_mae: 0.0329\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0013 - mae: 0.0262 - val_loss: 0.0016 - val_mae: 0.0305\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0013 - mae: 0.0261 - val_loss: 0.0016 - val_mae: 0.0309\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0012 - mae: 0.0258 - val_loss: 0.0014 - val_mae: 0.0283\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0012 - mae: 0.0253 - val_loss: 0.0020 - val_mae: 0.0360\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0014 - mae: 0.0275 - val_loss: 0.0023 - val_mae: 0.0408\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0012 - mae: 0.0249 - val_loss: 0.0013 - val_mae: 0.0263\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0013 - mae: 0.0262 - val_loss: 0.0014 - val_mae: 0.0276\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0012 - mae: 0.0257 - val_loss: 0.0013 - val_mae: 0.0265\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0012 - mae: 0.0251 - val_loss: 0.0017 - val_mae: 0.0333\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0011 - mae: 0.0247 - val_loss: 0.0019 - val_mae: 0.0344\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.0011 - mae: 0.0246 - val_loss: 0.0013 - val_mae: 0.0253\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0014 - mae: 0.0275 - val_loss: 0.0014 - val_mae: 0.0275\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0013 - mae: 0.0262 - val_loss: 0.0028 - val_mae: 0.0460\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0011 - mae: 0.0252 - val_loss: 0.0020 - val_mae: 0.0363\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0011 - mae: 0.0242 - val_loss: 0.0014 - val_mae: 0.0275\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0011 - mae: 0.0246 - val_loss: 0.0016 - val_mae: 0.0311\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0011 - mae: 0.0236 - val_loss: 0.0013 - val_mae: 0.0257\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0011 - mae: 0.0244 - val_loss: 0.0020 - val_mae: 0.0363\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0011 - mae: 0.0246 - val_loss: 0.0017 - val_mae: 0.0316\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.0011 - mae: 0.0238 - val_loss: 0.0019 - val_mae: 0.0344\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0011 - mae: 0.0238 - val_loss: 0.0013 - val_mae: 0.0257\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0010 - mae: 0.0233 - val_loss: 0.0013 - val_mae: 0.0263\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0011 - mae: 0.0238 - val_loss: 0.0025 - val_mae: 0.0415\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0012 - mae: 0.0253 - val_loss: 0.0013 - val_mae: 0.0266\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0011 - mae: 0.0244 - val_loss: 0.0017 - val_mae: 0.0327\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0011 - mae: 0.0258 - val_loss: 0.0023 - val_mae: 0.0401\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0010 - mae: 0.0238 - val_loss: 0.0013 - val_mae: 0.0269\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.8788e-04 - mae: 0.0228 - val_loss: 0.0020 - val_mae: 0.0364\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 9.5256e-04 - mae: 0.0231 - val_loss: 0.0016 - val_mae: 0.0308\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.4074e-04 - mae: 0.0230 - val_loss: 0.0017 - val_mae: 0.0313\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0010 - mae: 0.0239 - val_loss: 0.0016 - val_mae: 0.0301\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0011 - mae: 0.0253 - val_loss: 0.0015 - val_mae: 0.0296\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0011 - mae: 0.0250 - val_loss: 0.0012 - val_mae: 0.0236\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0010 - mae: 0.0242 - val_loss: 0.0030 - val_mae: 0.0476\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0011 - mae: 0.0253 - val_loss: 0.0018 - val_mae: 0.0331\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0012 - mae: 0.0264 - val_loss: 0.0011 - val_mae: 0.0218\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0013 - mae: 0.0289 - val_loss: 0.0012 - val_mae: 0.0244\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.6082e-04 - mae: 0.0230 - val_loss: 0.0018 - val_mae: 0.0329\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.2467e-04 - mae: 0.0217 - val_loss: 0.0014 - val_mae: 0.0273\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.7820e-04 - mae: 0.0222 - val_loss: 0.0020 - val_mae: 0.0348\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 9.2626e-04 - mae: 0.0228 - val_loss: 0.0012 - val_mae: 0.0240\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.9273e-04 - mae: 0.0239 - val_loss: 0.0011 - val_mae: 0.0231\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0011 - mae: 0.0256 - val_loss: 0.0023 - val_mae: 0.0396\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.2360e-04 - mae: 0.0231 - val_loss: 0.0015 - val_mae: 0.0298\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.2693e-04 - mae: 0.0215 - val_loss: 0.0012 - val_mae: 0.0246\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.0554e-04 - mae: 0.0230 - val_loss: 0.0018 - val_mae: 0.0332\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 9.2278e-04 - mae: 0.0235 - val_loss: 0.0011 - val_mae: 0.0227\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.6676e-04 - mae: 0.0221 - val_loss: 0.0013 - val_mae: 0.0259\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 8.3628e-04 - mae: 0.0220 - val_loss: 0.0024 - val_mae: 0.0395\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 9.5374e-04 - mae: 0.0238 - val_loss: 0.0012 - val_mae: 0.0242\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 9.5413e-04 - mae: 0.0232 - val_loss: 0.0025 - val_mae: 0.0423\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0011 - mae: 0.0252 - val_loss: 0.0011 - val_mae: 0.0228\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 8.9037e-04 - mae: 0.0224 - val_loss: 0.0012 - val_mae: 0.0252\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 8.5220e-04 - mae: 0.0218 - val_loss: 0.0017 - val_mae: 0.0318\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 8.7597e-04 - mae: 0.0219 - val_loss: 0.0011 - val_mae: 0.0231\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 8.7794e-04 - mae: 0.0226 - val_loss: 0.0014 - val_mae: 0.0284\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "verbose, epochs, batch_size = 1, 100, 32\n",
    "\n",
    "history = model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=epochs, batch_size=batch_size, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ba7dcb374143b38a510460ab053263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting a losses\n",
    "train_loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "plt.plot(train_loss, label=\"Training loss\", color='r')\n",
    "plt.plot(val_loss, label=\"Validation loss\", color='b')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Mean Square Error\")\n",
    "plt.title(\"Training & Validation Losses\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation metrics\n",
    "metrics = evaluate_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the test set\n",
    "prediction = list()\n",
    "\n",
    "yhat = model.predict(test_x)\n",
    "\n",
    "yhat = yhat.flatten()\n",
    "actual = test_y.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0028537415116063605\n",
      "RMSE: 0.05342042223350879\n",
      "MAE: 0.034424595693681184\n"
     ]
    }
   ],
   "source": [
    "#Output the error metrics\n",
    "print(f\"MSE: {metrics.mse(actual, yhat)}\")\n",
    "print(f\"RMSE: {metrics.rmse(actual, yhat)}\")\n",
    "print(f\"MAE: {metrics.mae(actual, yhat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0fffc84f2da42f088fc9e90e17a5bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close()\n",
    "plt.plot(yhat, label=\"Predicted\", color='r')\n",
    "plt.plot(actual, label='Actual', color='b')\n",
    "plt.xlabel(\"Time (h)\")\n",
    "plt.ylabel(\"Power\")\n",
    "plt.title(\"The predict of power consumption next 24 hours by used previous 24 hours\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
